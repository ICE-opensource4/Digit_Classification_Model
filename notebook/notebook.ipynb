{"cells":[{"cell_type":"markdown","metadata":{},"source":["## 1. Import libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import random\n","from tqdm import tqdm\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import DataLoader, Dataset, random_split\n","from torchvision.models import resnet34\n","from torchvision.transforms import v2"]},{"cell_type":"markdown","metadata":{},"source":["## 2. Set hyperparameters"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["batch_size = 1024\n","learning_rate = 1e-3\n","num_epochs = 1"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[],"source":["# Set seed\n","def set_seed(seed):\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.deterministic = True\n","    torch.backends.cudnn.benchmark = False\n","    np.random.seed(seed)\n","    random.seed(seed)\n","\n","\n","set_seed(42)"]},{"cell_type":"markdown","metadata":{},"source":["## 3. Data preprocessing"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[],"source":["# Read csv\n","train_data = pd.read_csv('../data/train.csv')\n","test_data = pd.read_csv('../data//test.csv')\n","submit = pd.read_csv('../data//sample_submission.csv')"]},{"cell_type":"code","execution_count":5,"metadata":{"trusted":true},"outputs":[],"source":["# Split train and validation data\n","train = np.array(train_data)\n","train_size = int(len(train) * 0.9)\n","valid_size = len(train) - train_size\n","\n","train_dataset, valid_dataset = random_split(train, (train_size, valid_size))"]},{"cell_type":"code","execution_count":6,"metadata":{"trusted":true},"outputs":[],"source":["# Split data and label\n","train_dataset = np.array(train_dataset)\n","valid_dataset = np.array(valid_dataset)\n","x_train = train_dataset[:, 1:]\n","x_valid = valid_dataset[:, 1:]\n","y_train = train_dataset[:, 0]\n","y_valid = valid_dataset[:, 0]\n","x_test = np.array(test_data)\n","\n","# Convert dtype to tensor and reshape data\n","x_train = torch.tensor(x_train, dtype=torch.float)\n","x_train = x_train.view(-1, 1, 28, 28)\n","x_valid = torch.tensor(x_valid, dtype=torch.float)\n","x_valid = x_valid.view(-1, 1, 28, 28)\n","x_test = torch.tensor(x_test, dtype=torch.float)\n","x_test = x_test.view(-1, 1, 28, 28)\n","\n","y_train = torch.tensor(y_train)\n","y_valid = torch.tensor(y_valid)\n","y_test = torch.randn(len(x_test), 10)\n","\n","# Transforms\n","train_transforms = v2.Compose(\n","    [\n","        v2.Resize(32, antialias=True),\n","        v2.RandomRotation(15),\n","        v2.ToDtype(torch.float32, scale=True),\n","        v2.Normalize(mean=[0.1307], std=[0.3081]),\n","    ]\n",")\n","test_transforms = v2.Compose(\n","    [\n","        v2.Resize(32, antialias=True),\n","        v2.ToDtype(torch.float32, scale=True),\n","        v2.Normalize(mean=[0.1307], std=[0.3081]),\n","    ]\n",")"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["# Define dataset\n","class MNIST(Dataset):\n","    def __init__(self, x_data, y_data, transforms):\n","        self.x_data = x_data\n","        self.y_data = y_data\n","        self.transforms = transforms\n","\n","    def __len__(self):\n","        return len(self.x_data)\n","\n","    def __getitem__(self, idx):\n","        img = self.x_data[idx]\n","        transformed_img = self.transforms(img)\n","        label = self.y_data[idx]\n","        return transformed_img, label"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":true},"outputs":[],"source":["# Load data\n","train_dataset = MNIST(x_train, y_train, train_transforms)\n","valid_dataset = MNIST(x_valid, y_valid, test_transforms)\n","test_dataset = MNIST(x_test, y_test, test_transforms)\n","\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"]},{"cell_type":"markdown","metadata":{},"source":["## 4. Define the model"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[],"source":["# Define CNN\n","class CNN(nn.Module):\n","    def __init__(self, num_classes):\n","        super(CNN, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 6, kernel_size=3, stride=1, padding=1)\n","        self.conv2 = nn.Conv2d(6, 16, kernel_size=3, stride=1, padding=1)\n","        self.conv3 = nn.Conv2d(16, 30, kernel_size=3, stride=1, padding=1)\n","        self.pool = nn.AvgPool2d(2, 2)\n","        self.relu = nn.ReLU()\n","        self.flatten = nn.Flatten()\n","        self.fc1 = nn.Linear(480, 84)\n","        self.fc2 = nn.Linear(84, num_classes)\n","\n","    def forward(self, x):\n","        x = self.conv1(x)\n","        x = self.relu(x)\n","        x = self.pool(x)\n","        x = self.conv2(x)\n","        x = self.relu(x)\n","        x = self.pool(x)\n","        x = self.conv3(x)\n","        x = self.relu(x)\n","        x = self.pool(x)\n","        x = self.flatten(x)\n","        x = self.fc1(x)\n","        x = self.relu(x)\n","        x = self.fc2(x)\n","        return x"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["# Can use cuda if you have gpu\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Load model, optimizer, loss function, learning rate scheduler\n","model = CNN(num_classes=10).to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","criterion = torch.nn.CrossEntropyLoss().to(device)\n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)"]},{"cell_type":"markdown","metadata":{},"source":["## 5. Train and validation"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["\n","Epoch : 1/1\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 37/37 [00:12<00:00,  3.04it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Train loss : 1.0770\n","Train acc : 0.6809\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 5/5 [00:00<00:00, 15.18it/s]"]},{"name":"stdout","output_type":"stream","text":["Valid loss : 0.2911\n","Valid acc : 0.9105\n","Best epoch : 1\n","Best acc : 0.9105\n","\n","Finished!\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Train\n","best_epoch = 1\n","best_acc = 0.0\n","for epoch in range(num_epochs):\n","    print(\"\" * 40)\n","    print(f\"Epoch : {epoch+1}/{num_epochs}\")\n","    epoch_loss = 0.0\n","    epoch_corrects = 0\n","    model.train()\n","    for batch_in, batch_out in tqdm(train_loader):\n","        batch_in = batch_in.to(device)\n","        batch_out = batch_out.to(device)\n","\n","        y_pred = model(batch_in)\n","        _, preds = torch.max(y_pred, 1)\n","\n","        loss = criterion(y_pred, batch_out)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","        epoch_loss += loss.item() * batch_in.size(0)\n","        epoch_corrects += torch.sum(preds == batch_out.data)\n","\n","    epoch_loss = epoch_loss / len(train_loader.dataset)\n","    epoch_acc = epoch_corrects.double() / len(train_loader.dataset)\n","\n","    print(f\"Train loss : {epoch_loss:.4f}\")\n","    print(f\"Train acc : {epoch_acc:.4f}\")\n","\n","# Validation\n","    epoch_loss = 0.0\n","    epoch_corrects = 0\n","    model.eval()\n","    for batch_in, batch_out in tqdm(valid_loader):\n","        batch_in = batch_in.to(device)\n","        batch_out = batch_out.to(device)\n","\n","        with torch.no_grad():\n","            y_pred = model(batch_in)\n","            _, preds = torch.max(y_pred, 1)\n","\n","            loss = criterion(y_pred, batch_out)\n","\n","            epoch_loss += loss.item() * batch_in.size(0)\n","            epoch_corrects += torch.sum(preds == batch_out.data)\n","\n","    epoch_loss = epoch_loss / len(valid_loader.dataset)\n","    epoch_acc = epoch_corrects.double() / len(valid_loader.dataset)\n","\n","    scheduler.step()\n","\n","    if epoch_acc >= best_acc:\n","        best_acc = epoch_acc\n","        best_epoch = epoch + 1\n","        torch.save(model, \"../checkpoints/model.pt\")\n","\n","    print(f\"Valid loss : {epoch_loss:.4f}\")\n","    print(f\"Valid acc : {epoch_acc:.4f}\")\n","    print(f\"Best epoch : {best_epoch}\")\n","    print(f\"Best acc : {best_acc:.4f}\")\n","    print(\"\" * 40)\n","\n","print(\"Finished!\")"]},{"cell_type":"markdown","metadata":{},"source":["## 6. Result"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 28/28 [00:02<00:00, 12.52it/s]\n"]}],"source":["# Get a result\n","model = torch.load(\"../checkpoints/model.pt\")\n","model.to(device)\n","\n","preds = []\n","with torch.no_grad():\n","    model.eval()\n","    for batch_in, batch_out in tqdm(test_loader):\n","        batch_in = batch_in.to(device)\n","        y_pred = model(batch_in)\n","        y_pred = torch.argmax(y_pred, 1)\n","        preds.extend(y_pred.cpu().numpy())\n","        \n","submit[\"Label\"] = preds\n","submit.to_csv(\"../predicts/predict.csv\", index=False)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"databundleVersionId":861823,"sourceId":3004,"sourceType":"competition"}],"dockerImageVersionId":30683,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.19"}},"nbformat":4,"nbformat_minor":4}
